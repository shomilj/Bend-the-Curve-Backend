{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "LINK_RECOVERED = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Recovered.csv\"\n",
    "LINK_DEATHS = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Deaths.csv\"\n",
    "LINK_CONFIRMED = \"https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_19-covid-Confirmed.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests, io\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_recovered_orig = pd.read_csv(io.StringIO(requests.get(LINK_RECOVERED).content.decode('utf-8')))\n",
    "df_deaths_orig = pd.read_csv(io.StringIO(requests.get(LINK_DEATHS).content.decode('utf-8')))\n",
    "df_confirmed_orig = pd.read_csv(io.StringIO(requests.get(LINK_CONFIRMED).content.decode('utf-8')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_recovered_orig = df_recovered_orig.append(df_recovered_orig.sum(numeric_only=True), ignore_index=True)\n",
    "df_recovered_orig.at[df_recovered_orig.index[-1], 'Country/Region'] = 'Global'\n",
    "df_recovered_orig.at[df_recovered_orig.index[-1], 'Province/State'] = 'Global'\n",
    "\n",
    "df_deaths_orig = df_deaths_orig.append(df_deaths_orig.sum(numeric_only=True), ignore_index=True)\n",
    "df_deaths_orig.at[df_deaths_orig.index[-1], 'Country/Region'] = 'Global'\n",
    "df_deaths_orig.at[df_deaths_orig.index[-1], 'Province/State'] = 'Global'\n",
    "\n",
    "df_confirmed_orig = df_confirmed_orig.append(df_confirmed_orig.sum(numeric_only=True), ignore_index=True)\n",
    "df_confirmed_orig.at[df_confirmed_orig.index[-1], 'Country/Region'] = 'Global'\n",
    "df_confirmed_orig.at[df_confirmed_orig.index[-1], 'Province/State'] = 'Global'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "today = df_recovered_orig.columns[-1]\n",
    "yesterday = df_recovered_orig.columns[-2]\n",
    "daybefore = df_recovered_orig.columns[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed_groups = df_confirmed_orig[(~pd.isnull(df_confirmed_orig['Province/State']))].groupby('Country/Region').sum().reset_index()\n",
    "recovered_groups = df_recovered_orig[(~pd.isnull(df_recovered_orig['Province/State']))].groupby('Country/Region').sum().reset_index()\n",
    "deaths_groups = df_deaths_orig[(~pd.isnull(df_deaths_orig['Province/State']))].groupby('Country/Region').sum().reset_index()\n",
    "\n",
    "df_confirmed = pd.concat([df_confirmed_orig, confirmed_groups], sort=False)\n",
    "df_deaths = pd.concat([df_deaths_orig, deaths_groups], sort=False)\n",
    "df_recovered = pd.concat([df_recovered_orig, recovered_groups], sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [df_confirmed, df_deaths, df_recovered]:\n",
    "    for i, row in list(df.iterrows()):\n",
    "        if row['Country/Region'] == row['Province/State']:\n",
    "            df.drop(i, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = df_confirmed.columns[-2]\n",
    "oneday = df_confirmed.columns[-3]\n",
    "fiveday = df_confirmed.columns[-8]\n",
    "tenday = df_confirmed.columns[-13]\n",
    "\n",
    "groups = [(\"oneDay\", oneday), (\"fiveDay\", fiveday), (\"tenDay\", tenday)]\n",
    "for g in groups:\n",
    "    label = g[0]\n",
    "    col = g[1]\n",
    "    for df in [df_recovered, df_deaths, df_confirmed]:\n",
    "        df[label + 'Num'] = df[today] - df[col]\n",
    "        df[label + 'Percent'] = 100 * ((df[today] - df[col]) / df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replacena(x):\n",
    "    if pd.isnull(x) or np.isnan(x):\n",
    "        return 0\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload = []\n",
    "upload_data = defaultdict(dict)\n",
    "\n",
    "for label, df in [('Confirmed', df_confirmed), ('Dead', df_deaths), ('Recovered', df_recovered)]:\n",
    "    for _, row in df.iterrows():\n",
    "        row = row.to_dict()\n",
    "        region = row.pop('Province/State')\n",
    "        country = row.pop('Country/Region')\n",
    "        key = str(region) + '__' + country\n",
    "        upload_data[key][label] = {\n",
    "            'lat': row.pop('Lat'),\n",
    "            'lon': row.pop('Long'),\n",
    "            'region': str(region),\n",
    "            'country': country,\n",
    "            'oneDayNum': replacena(row.pop('oneDayNum')),\n",
    "            'fiveDayNum': replacena(row.pop('fiveDayNum')),\n",
    "            'tenDayNum': replacena(row.pop('tenDayNum')),\n",
    "            'oneDayPercent': replacena(row.pop('oneDayPercent')),\n",
    "            'fiveDayPercent': replacena(row.pop('fiveDayPercent')),\n",
    "            'tenDayPercent': replacena(row.pop('tenDayPercent')),\n",
    "            'key': key\n",
    "        }\n",
    "        X_axis = list(row.keys())\n",
    "        y_axis = list(row.values())\n",
    "        upload_data[key][label]['X'] = X_axis\n",
    "        upload_data[key][label]['Y'] = y_axis\n",
    "        \n",
    "uploads = {}\n",
    "for key, dataset in upload_data.items():\n",
    "    confirmed_X = dataset['Confirmed'].pop('X')\n",
    "    confirmed_Y = dataset['Confirmed'].pop('Y')\n",
    "    deaths_Y = dataset['Dead'].pop('Y')    \n",
    "    recovered_Y = dataset['Recovered'].pop('Y')\n",
    "    \n",
    "    timeSeriesY = {'Confirmed': confirmed_Y,\n",
    "                  'Deaths': deaths_Y,\n",
    "                  'Recovered': recovered_Y}\n",
    "    \n",
    "    upload = dataset['Confirmed']\n",
    "    upload['timeSeriesX'] = confirmed_X\n",
    "    upload['timeSeriesY'] = timeSeriesY\n",
    "    upload['timeSeriesKeys'] = ['Confirmed', 'Deaths', 'Recovered']\n",
    "    uploads[key] = upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from firebase_admin import firestore, credentials\n",
    "import firebase_admin\n",
    "try:\n",
    "    cred = credentials.Certificate('/Users/shomil/Documents/ServerKeys/ncov19.json')\n",
    "    firebase_admin.initialize_app(cred)\n",
    "except Exception as e:\n",
    "    try:\n",
    "        firebase_admin.initialize_app()\n",
    "    except Exception as f:\n",
    "        print(\"Already initialized!\")\n",
    "        print(e, f)\n",
    "db = firestore.client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }, update_time {\n",
       "   seconds: 1585030757\n",
       "   nanos: 454110000\n",
       " }]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = db.batch()\n",
    "count = 0\n",
    "for k, v in uploads.items():\n",
    "    batch.set(db.collection('statistics').document(k), v)\n",
    "    count += 1\n",
    "    if count == 200:\n",
    "        batch.commit()\n",
    "        count = 0\n",
    "batch.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESSFULLY PUSHED DATA!\n"
     ]
    }
   ],
   "source": [
    "print(\"SUCCESSFULLY PUSHED DATA!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete_collection(db.collection('statistics'), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def delete_collection(coll_ref, batch_size):\n",
    "#     docs = coll_ref.limit(batch_size).stream()\n",
    "#     deleted = 0\n",
    "\n",
    "#     for doc in docs:\n",
    "#         print(u'Deleting doc {} => {}'.format(doc.id, doc.to_dict()))\n",
    "#         doc.reference.delete()\n",
    "#         deleted = deleted + 1\n",
    "\n",
    "#     if deleted >= batch_size:\n",
    "#         return delete_collection(coll_ref, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
